{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppo_continuous_actionpy\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tyro\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    exp_name: str = \"ppo_continuous_action\"\n",
    "    \"\"\"the name of this experiment\"\"\"\n",
    "    seed: int = 1\n",
    "    \"\"\"seed of the experiment\"\"\"\n",
    "    torch_deterministic: bool = True\n",
    "    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n",
    "    cuda: bool = True\n",
    "    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n",
    "    track: bool = False\n",
    "    \"\"\"if toggled, this experiment will be tracked with Weights and Biases\"\"\"\n",
    "    wandb_project_name: str = \"cleanRL\"\n",
    "    \"\"\"the wandb's project name\"\"\"\n",
    "    wandb_entity: str = None\n",
    "    \"\"\"the entity (team) of wandb's project\"\"\"\n",
    "    capture_video: bool = False\n",
    "    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n",
    "    save_model: bool = False\n",
    "    \"\"\"whether to save model into the `runs/{run_name}` folder\"\"\"\n",
    "    upload_model: bool = False\n",
    "    \"\"\"whether to upload the saved model to huggingface\"\"\"\n",
    "    hf_entity: str = \"\"\n",
    "    \"\"\"the user or org name of the model repository from the Hugging Face Hub\"\"\"\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    env_id: str = \"HalfCheetah-v4\"\n",
    "    \"\"\"the id of the environment\"\"\"\n",
    "    total_timesteps: int = 1000000\n",
    "    \"\"\"total timesteps of the experiments\"\"\"\n",
    "    learning_rate: float = 3e-4\n",
    "    \"\"\"the learning rate of the optimizer\"\"\"\n",
    "    num_envs: int = 1\n",
    "    \"\"\"the number of parallel game environments\"\"\"\n",
    "    num_steps: int = 2048\n",
    "    \"\"\"the number of steps to run in each environment per policy rollout\"\"\"\n",
    "    anneal_lr: bool = True\n",
    "    \"\"\"Toggle learning rate annealing for policy and value networks\"\"\"\n",
    "    gamma: float = 0.99\n",
    "    \"\"\"the discount factor gamma\"\"\"\n",
    "    gae_lambda: float = 0.95\n",
    "    \"\"\"the lambda for the general advantage estimation\"\"\"\n",
    "    num_minibatches: int = 32\n",
    "    \"\"\"the number of mini-batches\"\"\"\n",
    "    update_epochs: int = 10\n",
    "    \"\"\"the K epochs to update the policy\"\"\"\n",
    "    norm_adv: bool = True\n",
    "    \"\"\"Toggles advantages normalization\"\"\"\n",
    "    clip_coef: float = 0.2\n",
    "    \"\"\"the surrogate clipping coefficient\"\"\"\n",
    "    clip_vloss: bool = True\n",
    "    \"\"\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\"\"\"\n",
    "    ent_coef: float = 0.0\n",
    "    \"\"\"coefficient of the entropy\"\"\"\n",
    "    vf_coef: float = 0.5\n",
    "    \"\"\"coefficient of the value function\"\"\"\n",
    "    max_grad_norm: float = 0.5\n",
    "    \"\"\"the maximum norm for the gradient clipping\"\"\"\n",
    "    target_kl: float = None\n",
    "    \"\"\"the target KL divergence threshold\"\"\"\n",
    "\n",
    "    # to be filled in runtime\n",
    "    batch_size: int = 0\n",
    "    \"\"\"the batch size (computed in runtime)\"\"\"\n",
    "    minibatch_size: int = 0\n",
    "    \"\"\"the mini-batch size (computed in runtime)\"\"\"\n",
    "    num_iterations: int = 0\n",
    "    \"\"\"the number of iterations (computed in runtime)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, idx, capture_video, run_name, gamma):\n",
    "    def thunk():\n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        env = gym.wrappers.FlattenObservation(env)  # deal with dm_control's Dict observation space\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env = gym.wrappers.ClipAction(env)\n",
    "        env = gym.wrappers.NormalizeObservation(env)\n",
    "        env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))\n",
    "        env = gym.wrappers.NormalizeReward(env, gamma=gamma)\n",
    "        env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(-1.0, 1.0, (6,), float32)\n",
      "Box(-inf, inf, (17,), float64)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('HalfCheetah-v4')\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.22610849, -0.26133633, -0.9654431 ,  0.38364863,  0.97026044,\n",
       "        0.06519541], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous actions via normal distributions\n",
    "## why actor_logstd is state independent\n",
    "in the [doc](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#9-details-for-continuous-action-domains-eg-mujoco), says there are no major differences between dependent and independent \n",
    "## torch.expand_as\n",
    "\n",
    "```python\n",
    "import torch\n",
    "x = torch.tensor([[1], [2], [3]])\n",
    "print('x', x)\n",
    "y = torch.tensor([[2,2], [3,3], [5,5]])\n",
    "print('y', y)\n",
    "print('x.expand(3, 4): \\n', x.expand(3, 4))\n",
    "print('x.expand_as(y): \\n', x.expand_as(y))\n",
    "print('x is not changed: \\n',  x)\n",
    "```\n",
    "```shell\n",
    "x = torch.tensor([[1], [2], [3]])\n",
    "print('x', x)\n",
    "y = torch.tensor([[2,2], [3,3], [5,5]])\n",
    "print('y', y)\n",
    "print('x.expand(3, 4): \\n', x.expand(3, 4))\n",
    "print('x.expand_as(y): \\n', x.expand_as(y))\n",
    "print('x is not changed: \\n',  x)\n",
    "``` \n",
    "\n",
    "## np.sum(axis=xxx)\n",
    "Prompt: can you show me the pseudo code to implement this kind of `sum(axis:int|tuple[int...])`    \n",
    "\n",
    "Assuming array shape is ($4_0$, $3_1$, $2_2$),     \n",
    "`sum(axis=0)` the resulting shape will be `(3, 2)` like dimensionality reduction, like squeeze a cube(3d) `x,y,z` into plane(2d) (keep `x,y` or `x,z`, or `y,z`) the will expand somehow\n",
    "\n",
    "```python\n",
    "array = np.array([[[ 1,  2],\n",
    "                   [ 3,  4],\n",
    "                   [ 5,  6]],\n",
    "\n",
    "                  [[ 7,  8],\n",
    "                   [ 9, 10],\n",
    "                   [11, 12]],\n",
    "\n",
    "                  [[13, 14],\n",
    "                   [15, 16],\n",
    "                   [17, 18]],\n",
    "\n",
    "                  [[19, 20],\n",
    "                   [21, 22],\n",
    "                   [23, 24]]])\n",
    "\n",
    "\n",
    "sum_axis_1 = \n",
    "```\n",
    "\n",
    "`np.sum(array, axis=0)`    \n",
    "[    \n",
    "    [40, 44],  $$(1_{000}+7_{010}+13_{020}+19_{030}), (2_{001}+8_{011}+14_{021}+20_{031})$$    \n",
    "    [48, 52],  $$(3_{100}+9_{110}+15_{120}+21_{130}), (4_{101}+10_{111}+16_{121}+22_{131})$$    \n",
    "    [56, 60],  $$(5_{200}+11_{210}+17_{220}+23_{230}), (6_{201}+12_{211}+18_{221}+24_{231})$$    \n",
    "]\n",
    "\n",
    "`np.sum(array, axis=1)`\n",
    "[    \n",
    "    [ 6, 12],  $$(1_{000}+3_{100}+5_{200}), (2_{001}+4_{101}+6_{201})$$\n",
    "    [27, 30],  $$(7_{010}+9_{110}+11_{210}), (8_{011}+10_{111}+12_{211})$$\n",
    "    [45, 48],  $$(13_{020}+15_{120}+17_{220}), (14_{021}+16_{121}+18_{221})$$\n",
    "    [63, 66],  $$(19_{030}+21_{130}+23_{230}), (20_{031}+22_{131}+24_{231})$$\n",
    "]\n",
    "\n",
    "## np.prod(envs.single_action_space.shape) vs venvs.single_action_space.n\n",
    "Q: If the shape is (3,), `np.prod(envs.single_action_space.shape)` would return 3, then what is the `envs.single_action_space.n` in this case\n",
    "A: \n",
    "* If you have a continuous action space (like (3,) for three continuous values):\n",
    "np.prod(envs.single_action_space.shape) would return 3, reflecting the number of continuous outputs expected by the network.\n",
    "envs.single_action_space.n is not applicable and cannot be used(it typically raises an error or will return an undefined value.).\n",
    "* If you have a discrete action space:\n",
    "envs.single_action_space.n would *represent the total number of discrete actions available*.\n",
    "envs.single_action_space.shape would typically be () because it represents a single dimension without any additional shape information.\n",
    "\n",
    "## How is `actor_logstd` got learned?\n",
    "Q1: if `action_logstd = self.actor_logstd.expand_as(action_mean)` just a tensor.expand_as that changes the shape    \n",
    "A1:\n",
    "actor_logstd is initialized as a learnable parameter (using nn.Parameter) with a shape that matches the number of dimensions of the actions in the action space.\n",
    "\n",
    "Since actor_logstd is an nn.Parameter, it is included in the list of parameters that the optimizer will update during training.\n",
    "As the agent interacts with the environment and learns from its experiences (using techniques like policy gradients or other RL algorithms), the gradients will be backpropagated through the actor network, which includes the actor_logstd.\n",
    "This means that actor_logstd is actively learned and optimized based on the reward feedback it receives during training. It adjusts to enable the agent to learn better about how to distribute its actions based on the observations from the environment.\n",
    "\n",
    "\n",
    "Q2: then what is the  back propagation formula for this actor_logstd    \n",
    "A2:    \n",
    "In a reinforcement learning context, particularly in policy gradient methods such as those used in the CleanRL framework you've referenced, the backpropagation updates for parameters like `actor_logstd` involve computing gradients of the loss with respect to that parameter. \n",
    "\n",
    "### Overview of the Backpropagation Process\n",
    "\n",
    "$\\alpha$ In the provided code, `actor_logstd` is part of the actor network, which is used to model the policy distribution from which actions are sampled. The action distribution is parameterized by both the mean (`action_mean`) and the standard deviation (`action_std`). Here's a breakdown of how the backpropagation for `actor_logstd` occurs:\n",
    "\n",
    "1. **Loss Function**: \n",
    "   - In a policy gradient setup, the loss function for the policy network might typically involve the negative log probability of the actions taken, weighted by the rewards received (often using advantage estimates).\n",
    "   - A common loss function \\( L \\) might look similar to: \n",
    "   $$\n",
    "      L = -\\mathbb{E} \\left[ \\log(\\pi(a_t | s_t)) A_t \\right]\n",
    "   $$ \n",
    "   - where $\\pi(a_t | s_t)$ is the probability of taking action $a_t$ given state $s_t$, and \\( A_t \\) is the advantage at time \\( t \\).\n",
    "\n",
    "2. **Gradient Calculation**:\n",
    "   - The key is that when you compute the gradients of the loss with respect to `actor_logstd`, you will use the chain rule. \n",
    "\n",
    "3. **Chain Rule**:\n",
    "   - Let's denote the log probability of the action given the mean and standard deviation as:\n",
    "```math\n",
    "     \\log p(a | \\mu, \\sigma) = -\\frac{(a - \\mu)^2}{2\\sigma^2} - \\log(\\sigma \\sqrt{2\\pi})\n",
    "```\n",
    "   - When you differentiate the loss \\( L \\) with respect to `actor_logstd`, you are particularly interested in its contribution to the log probability of the actions taken and how it influences the overall loss function.\n",
    "\n",
    "### Backpropagation with Respect to `actor_logstd`\n",
    "\n",
    "Using the chain rule, the gradient of the loss with respect to `actor_logstd` can be expressed as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\text{actor\\_logstd}} = \\frac{\\partial L}{\\partial \\log p(a | \\mu, \\sigma)} \\cdot \\frac{\\partial \\log p(a | \\mu, \\sigma)}{\\partial \\text{actor\\_logstd}}\n",
    "$$\n",
    "\n",
    "Here are the steps broken down:\n",
    "\n",
    "1. **Computing $\\frac{\\partial L}{\\partial \\log p(a | \\mu, \\sigma)}$**:\n",
    "   - This term represents how the loss changes with respect to the log probabilities of the actions. This would involve the advantages, calculated during the training process.\n",
    "    \n",
    "2. **Computing $\\frac{\\partial \\log p(a | \\mu, \\sigma)}{\\partial \\text{actor\\_logstd}}$**:\n",
    "   - This can be computed from the log probability expression for the normal distribution:\n",
    "     $$\n",
    "     \\log p(a | \\mu, \\sigma) = -\\frac{(a - \\mu)^2}{2\\sigma^2} - \\log(\\sigma) - \\frac{1}{2}\\log(2\\pi)\n",
    "     $$\n",
    "   - Thus, the derivative with respect to `actor_logstd` is found as follows:\n",
    "     $$\n",
    "     \\frac{\\partial \\log p(a | \\mu, \\sigma)}{\\partial \\text{actor\\_logstd}} = -\\frac{(a - \\mu)^2 e^{-\\text{actor\\_logstd}}}{2} + \\frac{1}{\\text{actor\\_logstd}}\n",
    "     $$\n",
    "\n",
    "### Resulting Update\n",
    "\n",
    "The gradients computed from backpropagation are then used to update `actor_logstd` along with other parameters using an optimizer (like Adam or SGD). Given the optimizer's parameters such as the learning rate \\( \\alpha \\), the weight update for `actor_logstd` would look something like:\n",
    "\n",
    "$$\n",
    "\\text{actor\\_logstd} \\leftarrow \\text{actor\\_logstd} - \\alpha \\cdot \\frac{\\partial L}{\\partial \\text{actor\\_logstd}}\n",
    "$$\n",
    "\n",
    "### Summary\n",
    "\n",
    "In summary, during backpropagation:\n",
    "- The loss function is primarily influenced by the log probabilities of the actions given the states and the advantages from the rewards.\n",
    "- The gradients with respect to `actor_logstd` are derived through the chain rule and involve the parameters of the distribution (mean and standard deviation).\n",
    "- The learning process updates `actor_logstd` to improve the policy based on the actions taken and their rewards, enabling the agent to learn how to explore and exploit effectively. \n",
    "\n",
    "This mechanism allows `actor_logstd` to adapt over time, ensuring the action distribution can become more focused or varied as needed based on the agent's performance.\n",
    "## why probs.log_prob(action).sum(1) not sum(0)\n",
    "According to below equation\n",
    "$$\n",
    "\\displaystyle\\sum_{c=1}^{6} \\log \\mathcal{N}(\\mu_t^c, \\sigma_t^{c^2})(a_t^c) = \\log \\displaystyle\\prod_{c=1}^{6} \\mathcal{N}(\\mu_t^c, \\sigma_t^{c^2})(a_t^c) = \\log \\pi(a_t|s_t)\n",
    "$$\n",
    "\n",
    "```python\n",
    "arr = np.arange(24).reshape(4, 6)\n",
    "print(arr)\n",
    "print('======')\n",
    "print(arr.sum(axis=1))\n",
    "```\n",
    "\n",
    "```shell\n",
    "[[ 0  1  2  3  4  5] # sum(...) => 15\n",
    " [ 6  7  8  9 10 11] # sum(...) => 51\n",
    " [12 13 14 15 16 17] # sum(...) => 87\n",
    " [18 19 20 21 22 23]] # sum(...) => 123\n",
    "======\n",
    "[ 15  51  87 123]\n",
    "```\n",
    "\n",
    "## why `probs.log_prob(action).sum(axis=1)` not axis=other_axis\n",
    "### Understanding the Dimensions\n",
    "\n",
    "1. **Probabilities and Action Shapes**:\n",
    "   - When you create a distribution for actions using a normal distribution (as in your example), your `probs` object contains the means and standard deviations for each action dimension.\n",
    "   - The shape of `action_mean` and `action_std` will generally be `(batch_size, num_action_dims)` where:\n",
    "     - `batch_size` corresponds to the number of environments or the number of samples processed simultaneously.\n",
    "     - `num_action_dims` corresponds to the number of dimensions or components in the action space (e.g., a multi-dimensional continuous action).\n",
    "\n",
    "2. **Log Probability Calculation**:\n",
    "   - When you call `probs.log_prob(action)`, it computes the log probability of the `action` taken. Given the multi-dimensional nature of actions, the result would have the same shape as `action`, i.e., `(batch_size, num_action_dims)`.\n",
    "   - This gives a 2D tensor output where:\n",
    "     - Each row corresponds to the log probabilities for a particular `action` across all action dimensions for a sample in the batch.\n",
    "\n",
    "### Summation along Axis `1`\n",
    "\n",
    "- **`sum(1)`**:\n",
    "  - By summing along axis `1`, you're aggregating the log probabilities across all dimensions (or components) of the action for each sample in the batch.\n",
    "  - This means you will get a single log probability value per action taken for each sample, effectively collapsing the action dimensions into one value.\n",
    "  - For example, if `probs.log_prob(action)` yields a tensor of shape `(batch_size, num_action_dims)`, then `probs.log_prob(action).sum(1)` will yield a tensor of shape `(batch_size,)`, which represents the total log probability of the selected actions given the distribution.\n",
    "\n",
    "- **Interpretation**:\n",
    "  - This operation is essential for computing the total likelihood of the actions taken under the trained policy. In policy gradient methods, you typically want to maximize the likelihood/expected reward, which requires considering the total log probability associated with the chosen actions.\n",
    "\n",
    "## what does $\\log \\pi(a_t|s_t)$ mean\n",
    "Q: we got below equation for above context,\n",
    "$$\n",
    "\\displaystyle\\sum_{c=1}^{6} \\log \\mathcal{N}(\\mu_t^c, \\sigma_t^{c^2})(a_t^c) = \\log \\displaystyle\\prod_{c=1}^{6} \\mathcal{N}(\\mu_t^c, \\sigma_t^{c^2})(a_t^c) = \\log \\pi(a_t|s_t)\n",
    "$$\n",
    "\n",
    "what does $\\log \\pi(a_t|s_t)$ mean and  $Nu(\\mu_t^c, \\sigma_t^{c^2})(a_t^c)$, too?\n",
    "\n",
    "A:\n",
    "In the context of reinforcement learning and policy gradient methods, particularly when dealing with continuous action spaces, it's important to understand the notation used in equations related to probabilities and distributions. Let's break down the equation you provided:\n",
    "\n",
    "### Notation Breakdown\n",
    "\n",
    "1. **$\\log \\pi(a_t | s_t)$**:\n",
    "   - This represents the **log probability** of taking action $a_t$ given state $s_t$ according to the policy $\\pi$. In reinforcement learning, a policy defines the behavior of the agent—it specifies how the agent chooses its actions based on the current state.\n",
    "   - $\\pi(a_t | s_t)$ signifies the probability density function (PDF) that gives the likelihood of taking the specific action $a_t$ when in state $s_t$. This PDF could result from a normal distribution if we are modeling continuous actions.\n",
    "   - The logarithm of this probability is often used in training algorithms because it transforms the product of probabilities (a common occurrence when multiplying probabilities across multiple actions) into a sum, which simplifies calculations and can also help with numerical stability.\n",
    "\n",
    "2. **$Nu(\\mu_t^c, \\sigma_t^{c^2})(a_t^c)$**:\n",
    "   - Here, $Nu$ likely refers to a PDF (Probability Density Function) of a normal distribution, specifically denoted as $\\mathcal{N}$, which indicates that the action is being sampled from a normal distribution.\n",
    "   - The notation can be interpreted as:\n",
    "     $$\n",
    "     \\mathcal{N}(\\mu_t^c, \\sigma_t^{c^2})(a_t^c)\n",
    "     $$\n",
    "   - Where:\n",
    "     - $\\mu_t^c$ is the mean of the distribution for the $c$-th action component at time $t$.\n",
    "     - $\\sigma_t^2$ is the variance (the square of the standard deviation) of the distribution for the $c$-th action component at time $t$.\n",
    "     - $a_t^c$ is the specific action taken for the $c$-th action dimension at time $t$.\n",
    "   - So, this term gives the value of the probability density function for the normal distribution at the point defined by action $a_t^c$ given the mean and variance at that time.\n",
    "\n",
    "### Putting It All Together\n",
    "\n",
    "When you combine these ideas in the context of your equation:\n",
    "\n",
    "- The equation states that the log probability of the action $a_t$ being taken at state $s_t$ can be expressed as the sum of the log probabilities of the individual action components:\n",
    "\n",
    "$$\n",
    "\\sum_{c=1}^{6} \\log \\mathcal{N}(\\mu_t^c, \\sigma_t^{c^2})(a_t^c) = \\log \\prod_{c=1}^{6} \\mathcal{N}(\\mu_t^c, \\sigma_t^{c^2})(a_t^c)\n",
    "$$\n",
    "\n",
    "- On the left, you are summing the logs of the individual probabilities for each action component `c` (for instance, if you have an action space that is 6-dimensional, with each component modeled as a normal distribution).\n",
    "- On the right, you are using the property of logarithms that transforms the product of probabilities into a sum, highlighting how these components together contribute to the overall distribution, thereby representing the policy's probability for taking the combined action $a_t$ from state $s_t$.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- $\\log \\pi(a_t | s_t)$ represents the log probability of the action given the current state, allowing the agent to evaluate how good a decision it made.\n",
    "- $Nu(\\mu_t^c, \\sigma_t^{c^2})(a_t^c)$ indicates the probability density of selecting the action component $a_t^c$ based on its mean and variance, derived from a normal distribution, for each component of the action space.\n",
    "  \n",
    "These components are critical in understanding the dynamics of policy updates in reinforcement learning, specifically in continuous action spaces.\n",
    "\n",
    "## why Normal distribution\n",
    "Q: in continuous action context, why we choose normal distribution to sample action, why not other distributions?\n",
    "A:\n",
    "Choosing a normal (Gaussian) distribution to sample actions in continuous action settings in reinforcement learning has several advantages, but it is essential to consider the characteristics of the problem at hand. Below are some reasons why the normal distribution is commonly used, along with a discussion of other possible distributions.\n",
    "\n",
    "### Advantages of Using the Normal Distribution\n",
    "\n",
    "1. **Smoothness and Continuity**:\n",
    "   - The normal distribution is a continuous distribution that allows for smooth transitions in action values. This is particularly useful in environments where actions are continuous and need to change smoothly.\n",
    "\n",
    "2. **Analytic Properties**:\n",
    "   - The normal distribution has well-known mathematical properties that make it easier to work with mathematically. For example, it is fully characterized by just two parameters: mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)), which simplifies calculations involving gradients and expectations.\n",
    "\n",
    "3. **Handling of Uncertainty**:\n",
    "   - The normal distribution provides a natural way to model uncertainty in actions. The standard deviation controls the exploration vs. exploitation trade-off:\n",
    "     - A small standard deviation leads to taking actions close to the mean (exploitation), while a larger standard deviation encourages exploration of the action space.\n",
    "   \n",
    "4. **Gradient Calculations**:\n",
    "   - When using log probabilities of Gaussian distributions for policy gradients, the resulting gradients retain friendly mathematical forms. For actions taken from a Gaussian distribution, the gradient of the log probability with respect to the mean or variance can be computed easily, aiding in stable learning.\n",
    "\n",
    "5. **Sample Efficiency**:\n",
    "   - The structure of the normal distribution allows for effective exploration of the action space. In many cases, the Gaussian distribution can be tuned to quickly converge to optimal action strategies through mean and variance updates.\n",
    "\n",
    "6. **Central Limit Theorem**:\n",
    "   - The Central Limit Theorem suggests that many independent random variables will sum to a normally distributed variable when properly normalized. As a result, if action values are influenced by many factors, they may naturally follow a normal distribution.\n",
    "\n",
    "### Alternatives to Normal Distribution\n",
    "\n",
    "While the normal distribution is prevalent, other distributions can be useful, depending on the characteristics of the problem:\n",
    "\n",
    "1. **Uniform Distribution**:\n",
    "   - For tasks where any value within a specific range is equally likely (e.g., action spaces bounded within [0,1]), a uniform distribution might be more appropriate. However, uniform distributions do not have variance control like normal distributions.\n",
    "\n",
    "2. **Cauchy Distribution**:\n",
    "   - Used in situations where outliers are a significant concern, as the Cauchy distribution has heavier tails than the Gaussian, allowing for more exploration at the extremes.\n",
    "\n",
    "3. **Log-Normal Distribution**:\n",
    "   - When actions must always be positive, a log-normal distribution may be preferred, where the logarithm of the values follows a normal distribution. This is suitable for scenarios like position or length, where negative values don’t make sense.\n",
    "\n",
    "4. **Beta Distribution**:\n",
    "   - Often used for actions constrained between 0 and 1, particularly in environments with bounded continuous action spaces. The Beta distribution is flexible and can model various shapes based on its parameters.\n",
    "\n",
    "5. **Custom Distributions**:\n",
    "   - In complex environments, custom distributions can be constructed based on domain knowledge, potentially combining multiple distributions.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The choice of distribution for sampling actions in continuous settings largely depends on the problem characteristics and requirements:\n",
    "- **Normal distributions** are favored for their smoothness, simplicity, and effective way to explore multi-dimensional action spaces.\n",
    "- However, other distributions are certainly applicable and may be more suited to specific domains or constraints, and reinforcement learning researchers can choose or design a distribution based on the needs of their particular problem. \n",
    "\n",
    "Ultimately, normal distributions have become a standard choice largely due to their mathematical properties and the convenience they provide in many continuous action settings in reinforcement learning.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.10.12 (main, Aug  4 2023, 13:58:21) [Clang 14.0.3 (clang-1403.0.22.14.1)]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        self.actor_mean = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01),\n",
    "        )\n",
    "        self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(envs.single_action_space.shape)))\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        action_mean = self.actor_mean(x)\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(x)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args = tyro.cli(Args)\n",
    "args.batch_size = int(args.num_envs * args.num_steps)\n",
    "args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "args.num_iterations = args.total_timesteps // args.batch_size\n",
    "run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "if args.track:\n",
    "    import wandb\n",
    "\n",
    "    wandb.init(\n",
    "        project=args.wandb_project_name,\n",
    "        entity=args.wandb_entity,\n",
    "        sync_tensorboard=True,\n",
    "        config=vars(args),\n",
    "        name=run_name,\n",
    "        monitor_gym=True,\n",
    "        save_code=True,\n",
    "    )\n",
    "writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "writer.add_text(\n",
    "    \"hyperparameters\",\n",
    "    \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    ")\n",
    "\n",
    "# TRY NOT TO MODIFY: seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "# env setup\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(args.env_id, i, args.capture_video, run_name, args.gamma) for i in range(args.num_envs)]\n",
    ")\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Box), \"only continuous action space is supported\"\n",
    "\n",
    "agent = Agent(envs).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n",
    "\n",
    "# ALGO Logic: Storage setup\n",
    "obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n",
    "actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n",
    "logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "\n",
    "# TRY NOT TO MODIFY: start the game\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "next_obs, _ = envs.reset(seed=args.seed)\n",
    "next_obs = torch.Tensor(next_obs).to(device)\n",
    "next_done = torch.zeros(args.num_envs).to(device)\n",
    "\n",
    "for iteration in range(1, args.num_iterations + 1):\n",
    "    # Annealing the rate if instructed to do so.\n",
    "    if args.anneal_lr:\n",
    "        frac = 1.0 - (iteration - 1.0) / args.num_iterations\n",
    "        lrnow = frac * args.learning_rate\n",
    "        optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "    for step in range(0, args.num_steps):\n",
    "        global_step += args.num_envs\n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done\n",
    "\n",
    "        # ALGO LOGIC: action logic\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "            values[step] = value.flatten()\n",
    "        actions[step] = action\n",
    "        logprobs[step] = logprob\n",
    "\n",
    "        # TRY NOT TO MODIFY: execute the game and log data.\n",
    "        next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())\n",
    "        next_done = np.logical_or(terminations, truncations)\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)\n",
    "\n",
    "        if \"final_info\" in infos:\n",
    "            for info in infos[\"final_info\"]:\n",
    "                if info and \"episode\" in info:\n",
    "                    print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
    "                    writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "                    writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "\n",
    "    # bootstrap value if not done\n",
    "    with torch.no_grad():\n",
    "        next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "        advantages = torch.zeros_like(rewards).to(device)\n",
    "        lastgaelam = 0\n",
    "        for t in reversed(range(args.num_steps)):\n",
    "            if t == args.num_steps - 1:\n",
    "                nextnonterminal = 1.0 - next_done\n",
    "                nextvalues = next_value\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - dones[t + 1]\n",
    "                nextvalues = values[t + 1]\n",
    "            delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
    "            advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
    "        returns = advantages + values\n",
    "\n",
    "    # flatten the batch\n",
    "    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_logprobs = logprobs.reshape(-1)\n",
    "    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "    b_advantages = advantages.reshape(-1)\n",
    "    b_returns = returns.reshape(-1)\n",
    "    b_values = values.reshape(-1)\n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    b_inds = np.arange(args.batch_size)\n",
    "    clipfracs = []\n",
    "    for epoch in range(args.update_epochs):\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, args.batch_size, args.minibatch_size):\n",
    "            end = start + args.minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "\n",
    "            _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])\n",
    "            logratio = newlogprob - b_logprobs[mb_inds]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                old_approx_kl = (-logratio).mean()\n",
    "                approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "\n",
    "            mb_advantages = b_advantages[mb_inds]\n",
    "            if args.norm_adv:\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = -mb_advantages * ratio\n",
    "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            newvalue = newvalue.view(-1)\n",
    "            if args.clip_vloss:\n",
    "                v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                    newvalue - b_values[mb_inds],\n",
    "                    -args.clip_coef,\n",
    "                    args.clip_coef,\n",
    "                )\n",
    "                v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                v_loss = 0.5 * v_loss_max.mean()\n",
    "            else:\n",
    "                v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "        if args.target_kl is not None and approx_kl > args.target_kl:\n",
    "            break\n",
    "\n",
    "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "    # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "    writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "    writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "    writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "    writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "    writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "    print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "    writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "\n",
    "if args.save_model:\n",
    "    model_path = f\"runs/{run_name}/{args.exp_name}.cleanrl_model\"\n",
    "    torch.save(agent.state_dict(), model_path)\n",
    "    print(f\"model saved to {model_path}\")\n",
    "    from cleanrl_utils.evals.ppo_eval import evaluate\n",
    "\n",
    "    episodic_returns = evaluate(\n",
    "        model_path,\n",
    "        make_env,\n",
    "        args.env_id,\n",
    "        eval_episodes=10,\n",
    "        run_name=f\"{run_name}-eval\",\n",
    "        Model=Agent,\n",
    "        device=device,\n",
    "        gamma=args.gamma,\n",
    "    )\n",
    "    for idx, episodic_return in enumerate(episodic_returns):\n",
    "        writer.add_scalar(\"eval/episodic_return\", episodic_return, idx)\n",
    "\n",
    "    if args.upload_model:\n",
    "        from cleanrl_utils.huggingface import push_to_hub\n",
    "\n",
    "        repo_name = f\"{args.env_id}-{args.exp_name}-seed{args.seed}\"\n",
    "        repo_id = f\"{args.hf_entity}/{repo_name}\" if args.hf_entity else repo_name\n",
    "        push_to_hub(args, episodic_returns, repo_id, \"PPO\", f\"runs/{run_name}\", f\"videos/{run_name}-eval\")\n",
    "\n",
    "envs.close()\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cleanrl-TlZg4FDS-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
