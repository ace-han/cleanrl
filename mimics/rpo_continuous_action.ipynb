{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are the differences between rpo_continuous_action and ppo_continuous_action\n",
    "In the context of CleanRL, which is a library designed for implementing various reinforcement learning algorithms, you'll find different implementations of Proximal Policy Optimization (PPO) adapted for different types of action spaces. The two specific algorithms you've mentioned, `rpo_continuous_action` and `ppo_continuous_action`, represent different variations of the PPO algorithm for handling continuous action spaces.\n",
    "\n",
    "### Differences Between `rpo_continuous_action` and `ppo_continuous_action`\n",
    "\n",
    "1. **Algorithm Variants**:\n",
    "   - **`ppo_continuous_action`**: This implementation follows the standard PPO methodology as introduced by Schulman et al. in their original paper on PPO. PPO uses a clipped objective to provide a stable method for training policies by preventing large updates that will lead to erratic behavior.\n",
    "   - **`rpo_continuous_action`**: This stands for **\"Regularized PPO\"** or **\"Robust PPO,\"** depending on the specific implementation. This variant introduces additional regularization techniques designed to enhance stability and robustness during training compared to standard PPO. \n",
    "\n",
    "2. **Clipping and Objectives**:\n",
    "   - In **PPO**, the typical objective function includes a clipping mechanism that limits the policy's update step, preventing dramatic shifts in the policy distribution:\n",
    "     $$\n",
    "     L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t \\right) \\right]\n",
    "     $$\n",
    "     where $r_t(\\theta)$ is the probability ratio of the new and old policies, and $\\hat{A}_t$ is the estimated advantage.\n",
    "\n",
    "   - In **RPO**, additional regularization techniques may be added that further constrain the policy updates to encourage exploration while ensuring that the updates align with the underlying policy's behavior.\n",
    "\n",
    "3. **Exploration Techniques**:\n",
    "   - **PPO** generally relies on the regular policy gradient updates with the objective of maximizing expected rewards, focusing on balancing exploration and exploitation.\n",
    "   - **RPO** variations might include stronger exploration incentives or tailored advantage estimators that improve the agent's capacity to adapt to non-stationary or complex environments.\n",
    "\n",
    "4. **Implementation Specifics**:\n",
    "   - The actual implementation of these algorithms may differ in terms of code structure, logging, hyperparameter settings, and specific tricks employed, like normalization of state/action spaces, variance reduction techniques, and so forth.\n",
    "\n",
    "5. **Performance and Robustness**:\n",
    "   - Users may see differing performance results in different environments, with RPO potentially providing better robustness against instability during training, especially in environments with more complex dynamics or high variance in rewards.\n",
    "\n",
    "### Summary\n",
    "\n",
    "While both `rpo_continuous_action` and `ppo_continuous_action` are based on the Proximal Policy Optimization framework, the key differences are in the additional modifications and regularizations applied in RPO to achieve better stability and robustness during training. \n",
    "\n",
    "When choosing between them, the best approach might depend on the specific characteristics of the environment you're training the agent in, along with empirical results from performance testing. If you have access to both implementations in CleanRL, experimenting with both could provide insights into which algorithm performs better for your specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The different part\n",
    "The code snippet you've provided highlights a key difference between the `rpo_continuous_action` and `ppo_continuous_action` implementations in terms of how actions are sampled, particularly in the **RPO** (Regularized Proximal Optimization) approach. Letâ€™s break down the rationale and functionality behind the changes introduced in the RPO implementation.\n",
    "\n",
    "### Key Differences in Action Sampling\n",
    "\n",
    "1. **Standard PPO Behavior**: \n",
    "   - In the standard **PPO** implementation:\n",
    "     ```python\n",
    "     if action is None:\n",
    "         action = probs.sample()\n",
    "     ```\n",
    "   - Here, if no action is provided, a new action is sampled directly from the probability distribution defined by `action_mean` and `action_std`. This is typical in reinforcement learning, especially during training, where you want to sample actions that reflect the policy.\n",
    "\n",
    "2. **RPO Behavior**:\n",
    "   - In the **RPO** implementation:\n",
    "     ```python\n",
    "     else:  # new to RPO\n",
    "         # sample again to add stochasticity to the policy\n",
    "         z = torch.FloatTensor(action_mean.shape).uniform_(-self.rpo_alpha, self.rpo_alpha).to(device)\n",
    "         action_mean = action_mean + z\n",
    "         probs = Normal(action_mean, action_std)\n",
    "     ```\n",
    "   - In this case, if an action is provided (typically when you want to evaluate the action rather than sample), the method adds a stochastic perturbation to the `action_mean`.\n",
    "\n",
    "### Explanation of the RPO Regulation\n",
    "\n",
    "**Stochastic Perturbation**:\n",
    "- The line `z = torch.FloatTensor(action_mean.shape).uniform_(-self.rpo_alpha, self.rpo_alpha).to(device)` generates a tensor `z` filled with uniform noise drawn from the range $[-self.rpo_alpha, self.rpo_alpha]$. This noise is then added to the `action_mean`:\n",
    "  ```python\n",
    "  action_mean = action_mean + z\n",
    "  ```\n",
    "  \n",
    "- This perturbation introduces a degree of randomness into the action mean, which broadens the exploratory behavior of the policy. Essentially, the RPO approach is encouraging exploration even when a specific action (perhaps from a previous policy iteration) is provided.\n",
    "\n",
    "### Rationale Behind This Stochasticity\n",
    "- **Increased Exploration**: The added stochasticity ensures that the policy doesn't become too deterministic over time, which could happen if actions are chosen based purely on the computed mean without any variability. \n",
    "- **Regularization Effect**: By injecting noise into the action means, the RPO implementation can be considered as a form of **regularization**. It encourages exploration in a controlled manner and helps prevent overfitting to the current policy by sampling actions that are slightly different from what the model predicts as optimal.\n",
    "- **Adaptive Exploration**: The parameter `self.rpo_alpha` can control the amount of exploration. A higher value yields more exploration, while a lower value may lead to more exploitation based on what the policy has learned so far.\n",
    "\n",
    "### Summary\n",
    "The regulation and exploration strategy embedded in the `rpo_continuous_action` stems from adding controlled stochasticity to the action mean. This contrasts with the standard PPO implementation, where actions are sampled directly without any additional noise. This method helps to keep exploration alive and can improve the training stability and performance of the agent in challenging environments. \n",
    "\n",
    "In the context of Reinforcement Learning, particularly in stochastic environments or those with high-dimensional action spaces, such techniques can contribute significantly towards achieving a more robust learning process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why this stochastic perturbation not get clipped by ppo original algorithm?\n",
    "In Proximal Policy Optimization (PPO), the clipping mechanism is designed to prevent overly large updates to the policy during training by bounding the probability ratios. The core idea is to maintain a balance between exploration (updating the policy to discover new strategies) and stability (not moving too far away from the existing policy in a single update).\n",
    "\n",
    "### Understanding the Clipping Mechanism\n",
    "\n",
    "The formula you've provided reflects the PPO objective, which uses the following components:\n",
    "\n",
    "- $r_t(\\theta) = \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)}$\n",
    "  - This is the probability ratio of the new policy to the old policy for the action taken at time $t$.\n",
    "\n",
    "- $\\hat{A}_t$\n",
    "  - This represents the estimated advantage at time $t$, indicating how much better the action was compared to the policy's average action.\n",
    "\n",
    "- The clipping term:\n",
    "  $$\n",
    "  \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon)\n",
    "  $$\n",
    "  - This restricts the probability ratio to a range around 1 (essentially $[1 - \\epsilon, 1 + \\epsilon]$), which stabilizes training by limiting how much the policy can change in a single training step.\n",
    "\n",
    "### Interaction with Stochastic Perturbation\n",
    "\n",
    "1. **Stochastic Perturbation in RPO**:\n",
    "   - By adding stochastic perturbations to the `action_mean`, the policy is encouraged to sample actions that are distributed around the mean value, effectively injecting noise into the action selection.\n",
    "   - For example, if the original action mean is adjusted by some randomness, this affects the distribution but does not directly alter the policy's parameters. The sampling process is treated as part of the exploration.\n",
    "\n",
    "2. **Behaviors during Update**:\n",
    "   - The perturbation in `action_mean` does not cause the probability ratio $r_t(\\theta)$ to exceed the clipping limits because the perturbation is applied before calculating the action probabilities from the policy during that specific iteration.\n",
    "   - In other words, the probabilistic output (action distribution) remains bounded and represents a continuous action space governed by the standard deviation and the perturbed mean, which modifies the action taken without directly affecting the learned policy.\n",
    "\n",
    "3. **Clipping Effectiveness**:\n",
    "   - The clipping mechanism targets the ratio of the new policy to the old policy, which reflects how the policy changes respond to the actions taken during updates.\n",
    "   - The stochastic nature of the perturbation improves exploration without contradicting the clipping strategy. As long as the mean remains close to the previously learned mean (and the variances are reasonable), the ratio $r_t(\\theta)$ should not lead to large changes.\n",
    "\n",
    "### Summary\n",
    "\n",
    "The stochastic perturbation used in the **RPO** variant adds exploration but does not exceed the bounds imposed by the PPO clipping mechanism because:\n",
    "- **Noise Changes Action Distribution, Not Policy**: The perturbation modifies the actual sampled actions rather than the fundamental policy parameters governing $r_t(\\theta)$.\n",
    "- **Well-Bounded Exploration**: The probability distributions from which actions are sampled remain stable, adhering to the policy's learned behaviors, and generally do not yield large ratios that would exceed the clipping threshold.\n",
    "- **Training Stability**: The PPO algorithm's design allows these improvements in exploration to occur while preserving overall stability and convergence by clipping and regularly updating according to the policy's performance.\n",
    "\n",
    "Thus, the integration of stochastic perturbation in the RPO implementation enhances exploration capability while conforming to the safety bounds established by the clipping mechanism in PPO.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/rpo/#rpo_continuous_actionpy\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tyro\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    exp_name: str = os.path.basename(__file__)[: -len(\".py\")]\n",
    "    \"\"\"the name of this experiment\"\"\"\n",
    "    seed: int = 1\n",
    "    \"\"\"seed of the experiment\"\"\"\n",
    "    torch_deterministic: bool = True\n",
    "    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n",
    "    cuda: bool = True\n",
    "    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n",
    "    track: bool = False\n",
    "    \"\"\"if toggled, this experiment will be tracked with Weights and Biases\"\"\"\n",
    "    wandb_project_name: str = \"cleanRL\"\n",
    "    \"\"\"the wandb's project name\"\"\"\n",
    "    wandb_entity: str = None\n",
    "    \"\"\"the entity (team) of wandb's project\"\"\"\n",
    "    capture_video: bool = False\n",
    "    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    env_id: str = \"HalfCheetah-v4\"\n",
    "    \"\"\"the id of the environment\"\"\"\n",
    "    total_timesteps: int = 8000000\n",
    "    \"\"\"total timesteps of the experiments\"\"\"\n",
    "    learning_rate: float = 3e-4\n",
    "    \"\"\"the learning rate of the optimizer\"\"\"\n",
    "    num_envs: int = 1\n",
    "    \"\"\"the number of parallel game environments\"\"\"\n",
    "    num_steps: int = 2048\n",
    "    \"\"\"the number of steps to run in each environment per policy rollout\"\"\"\n",
    "    anneal_lr: bool = True\n",
    "    \"\"\"Toggle learning rate annealing for policy and value networks\"\"\"\n",
    "    gamma: float = 0.99\n",
    "    \"\"\"the discount factor gamma\"\"\"\n",
    "    gae_lambda: float = 0.95\n",
    "    \"\"\"the lambda for the general advantage estimation\"\"\"\n",
    "    num_minibatches: int = 32\n",
    "    \"\"\"the number of mini-batches\"\"\"\n",
    "    update_epochs: int = 10\n",
    "    \"\"\"the K epochs to update the policy\"\"\"\n",
    "    norm_adv: bool = True\n",
    "    \"\"\"Toggles advantages normalization\"\"\"\n",
    "    clip_coef: float = 0.2\n",
    "    \"\"\"the surrogate clipping coefficient\"\"\"\n",
    "    clip_vloss: bool = True\n",
    "    \"\"\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\"\"\"\n",
    "    ent_coef: float = 0.0\n",
    "    \"\"\"coefficient of the entropy\"\"\"\n",
    "    vf_coef: float = 0.5\n",
    "    \"\"\"coefficient of the value function\"\"\"\n",
    "    max_grad_norm: float = 0.5\n",
    "    \"\"\"the maximum norm for the gradient clipping\"\"\"\n",
    "    target_kl: float = None\n",
    "    \"\"\"the target KL divergence threshold\"\"\"\n",
    "    rpo_alpha: float = 0.5\n",
    "    \"\"\"the alpha parameter for RPO\"\"\"\n",
    "\n",
    "    # to be filled in runtime\n",
    "    batch_size: int = 0\n",
    "    \"\"\"the batch size (computed in runtime)\"\"\"\n",
    "    minibatch_size: int = 0\n",
    "    \"\"\"the mini-batch size (computed in runtime)\"\"\"\n",
    "    num_iterations: int = 0\n",
    "    \"\"\"the number of iterations (computed in runtime)\"\"\"\n",
    "\n",
    "\n",
    "def make_env(env_id, idx, capture_video, run_name, gamma):\n",
    "    def thunk():\n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        env = gym.wrappers.FlattenObservation(env)  # deal with dm_control's Dict observation space\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env = gym.wrappers.ClipAction(env)\n",
    "        env = gym.wrappers.NormalizeObservation(env)\n",
    "        env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))\n",
    "        env = gym.wrappers.NormalizeReward(env, gamma=gamma)\n",
    "        env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, rpo_alpha):\n",
    "        super().__init__()\n",
    "        self.rpo_alpha = rpo_alpha\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        self.actor_mean = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01),\n",
    "        )\n",
    "        self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(envs.single_action_space.shape)))\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        action_mean = self.actor_mean(x)\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        else:  # new to RPO\n",
    "            # sample again to add stochasticity to the policy\n",
    "            z = torch.FloatTensor(action_mean.shape).uniform_(-self.rpo_alpha, self.rpo_alpha).to(device)\n",
    "            action_mean = action_mean + z\n",
    "            probs = Normal(action_mean, action_std)\n",
    "\n",
    "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(x)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = tyro.cli(Args)\n",
    "    args.batch_size = int(args.num_envs * args.num_steps)\n",
    "    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "    args.num_iterations = args.total_timesteps // args.batch_size\n",
    "    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "    if args.track:\n",
    "        import wandb\n",
    "\n",
    "        wandb.init(\n",
    "            project=args.wandb_project_name,\n",
    "            entity=args.wandb_entity,\n",
    "            sync_tensorboard=True,\n",
    "            config=vars(args),\n",
    "            name=run_name,\n",
    "            monitor_gym=True,\n",
    "            save_code=True,\n",
    "        )\n",
    "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "    writer.add_text(\n",
    "        \"hyperparameters\",\n",
    "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    "    )\n",
    "\n",
    "    # TRY NOT TO MODIFY: seeding\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "    # env setup\n",
    "    envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(args.env_id, i, args.capture_video, run_name, args.gamma) for i in range(args.num_envs)]\n",
    "    )\n",
    "    assert isinstance(envs.single_action_space, gym.spaces.Box), \"only continuous action space is supported\"\n",
    "\n",
    "    agent = Agent(envs, args.rpo_alpha).to(device)\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n",
    "\n",
    "    # ALGO Logic: Storage setup\n",
    "    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n",
    "    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n",
    "    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "\n",
    "    # TRY NOT TO MODIFY: start the game\n",
    "    global_step = 0\n",
    "    start_time = time.time()\n",
    "    next_obs, _ = envs.reset(seed=args.seed)\n",
    "    next_obs = torch.Tensor(next_obs).to(device)\n",
    "    next_done = torch.zeros(args.num_envs).to(device)\n",
    "    num_updates = args.total_timesteps // args.batch_size\n",
    "\n",
    "    for update in range(1, num_updates + 1):\n",
    "        # Annealing the rate if instructed to do so.\n",
    "        if args.anneal_lr:\n",
    "            frac = 1.0 - (update - 1.0) / num_updates\n",
    "            lrnow = frac * args.learning_rate\n",
    "            optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "        for step in range(0, args.num_steps):\n",
    "            global_step += 1 * args.num_envs\n",
    "            obs[step] = next_obs\n",
    "            dones[step] = next_done\n",
    "\n",
    "            # ALGO LOGIC: action logic\n",
    "            with torch.no_grad():\n",
    "                action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "                values[step] = value.flatten()\n",
    "            actions[step] = action\n",
    "            logprobs[step] = logprob\n",
    "\n",
    "            # TRY NOT TO MODIFY: execute the game and log data.\n",
    "            next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())\n",
    "            done = np.logical_or(terminations, truncations)\n",
    "            rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "\n",
    "            if \"final_info\" in infos:\n",
    "                for info in infos[\"final_info\"]:\n",
    "                    if info and \"episode\" in info:\n",
    "                        print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
    "                        writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "                        writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "\n",
    "        # bootstrap value if not done\n",
    "        with torch.no_grad():\n",
    "            next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "            advantages = torch.zeros_like(rewards).to(device)\n",
    "            lastgaelam = 0\n",
    "            for t in reversed(range(args.num_steps)):\n",
    "                if t == args.num_steps - 1:\n",
    "                    nextnonterminal = 1.0 - next_done\n",
    "                    nextvalues = next_value\n",
    "                else:\n",
    "                    nextnonterminal = 1.0 - dones[t + 1]\n",
    "                    nextvalues = values[t + 1]\n",
    "                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
    "                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
    "            returns = advantages + values\n",
    "\n",
    "        # flatten the batch\n",
    "        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "        b_logprobs = logprobs.reshape(-1)\n",
    "        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "        b_advantages = advantages.reshape(-1)\n",
    "        b_returns = returns.reshape(-1)\n",
    "        b_values = values.reshape(-1)\n",
    "\n",
    "        # Optimizing the policy and value network\n",
    "        b_inds = np.arange(args.batch_size)\n",
    "        clipfracs = []\n",
    "        for epoch in range(args.update_epochs):\n",
    "            np.random.shuffle(b_inds)\n",
    "            for start in range(0, args.batch_size, args.minibatch_size):\n",
    "                end = start + args.minibatch_size\n",
    "                mb_inds = b_inds[start:end]\n",
    "\n",
    "                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])\n",
    "                logratio = newlogprob - b_logprobs[mb_inds]\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "\n",
    "                mb_advantages = b_advantages[mb_inds]\n",
    "                if args.norm_adv:\n",
    "                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "                # Policy loss\n",
    "                pg_loss1 = -mb_advantages * ratio\n",
    "                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # Value loss\n",
    "                newvalue = newvalue.view(-1)\n",
    "                if args.clip_vloss:\n",
    "                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                    v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                        newvalue - b_values[mb_inds],\n",
    "                        -args.clip_coef,\n",
    "                        args.clip_coef,\n",
    "                    )\n",
    "                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                    v_loss = 0.5 * v_loss_max.mean()\n",
    "                else:\n",
    "                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "            if args.target_kl is not None:\n",
    "                if approx_kl > args.target_kl:\n",
    "                    break\n",
    "\n",
    "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "        # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "        writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "        writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "        writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "        print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "        writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "\n",
    "    envs.close()\n",
    "    writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cleanrl-TlZg4FDS-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
